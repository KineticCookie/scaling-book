---
layout: distill
title: "Все о Rooflines"
# permalink: /main/
description: "Когда мы запускаем алгоритмы на железе, нас ограничивают три вещи: скорость вычислений (операций/сек), пропускная способность для перемещения данных (байт/сек) и общий объём доступной памяти для хранения данных (байты). Эти ограничения (roofline) позволяют нам оценить верхнюю и нижнюю границы времени выполнения конкретных вычислений.
"
date: 2025-02-04
future: true
htmlwidgets: true
hidden: false

section_number: 1

previous_section_url: ".."
previous_section_name: "Часть 0: Введение"

next_section_url: ../tpus
next_section_name: "Часть 2: TPU"

bibliography: main.bib

giscus_comments: true

authors:
  - name: Jacob Austin
    url: "https://www.jacobaustin.org/"
    affiliations:
      name: Google DeepMind
  - name: Sholto Douglas
    url: "https://x.com/_sholtodouglas"
  - name: Roy Frostig
    url: "https://cs.stanford.edu/~rfrostig/"
  - name: Anselm Levskaya
    url: "https://anselmlevskaya.com/"
  - name: Charlie Chen
    url: "https://x.com/charliexychen"
  - name: Sharad Vikram
    url: "https://sharadvikram.com/"
  - name: Federico Lebron
    url: "https://fedelebron.com/"
  - name: Peter Choy
    url: "https://x.com/pchoy95"
  - name: Vinay Ramasesh
    url: "https://x.com/vinayramasesh"
  - name: Albert Webson
    url: "https://representation.ai/"
  - name: Reiner Pope<sup>*</sup>
    url: https://x.com/reinerpope

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - please use this format rather than manually creating a markdown table of contents.
toc:

  - name: Куда уходит время?
  - subsections:
    - name: "Визуализация roofline-графиков"
    - name: "Матричное умножение"
    - name: "Roofline для сетевой коммуникации"
  - name: Несколько задач для практики

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Куда уходит время?

Начнём с очень простого вопроса: *почему алгоритм выполняется 50 миллисекунд, а не 50 секунд или 5 миллисекунд?* Что именно происходит внутри модели, что занимает столько времени, и сколько это вообще должно занимать?

> **От переводчика:** Возможна путаница связанная с FLOPs. В оригинале используется термин "FLOPs", который является множественным числом от "FLOP" (floating point operation). То есть если у алгоритма 5 операций сложения с плавающей точкой, то у него 5 FLOPs. Проблема в том что в интернете часто используют "FLOPS" (floating point operations per second) для обозначения количества операций в секунду, когда говорят о производительности железа. С непривычки это может сбивать с толку потому что в книге производительность пишут через FLOPs/s.
>
> То есть в нотации этой книги:
>
> - **FLOPs** — **количество** операций с плавающей точкой (например, сложность алгоритма -- 1e12 FLOPs)
> - **FLOPs/s** — количество операций с плавающей точкой **в секунду** (например, производительность NVIDIA GeForce RTX 4070 Ti SUPER
--  44.10e12 FLOPs/s)
> - FLOPS - не используется в этой книге, чтобы избежать путаницы. Вместо этого будет использоваться FLOPs/s.

**Вычисления:** По сути, модель глубокого обучения — это куча матричных умножений, каждое из которых состоит из операций умножения и сложения с плавающей точкой (FLOPs). Скорость нашего ускорителя определяет, сколько времени займут эти вычисления:

$$\begin{equation}
T_\text{math} = \frac{\text{Вычислительная сложность (FLOPs)}}{\text{Производительность ускорителя (FLOPs/s)}}
\end{equation}$$

Например, NVIDIA H100 выполняет примерно 9.89e14 операций bfloat16<d-footnote>bf16 — это сокращение от <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">bfloat16</a>, 16-битного формата с плавающей точкой, который часто используется в ML.</d-footnote> в секунду, а TPU v6e — 9.1e14 FLOPs/s.<d-footnote>H100 и B200 обычно достигают лишь 80-85% от заявленной пиковой производительности, в то время как TPU при обычном использовании могут выйти на 95%.</d-footnote> Это значит, что выполнение 1e12 FLOPs на H100 займёт примерно `1e12 / 9.89e14 = 1.01 мс`, а на TPU v6e — `1e12 / 9.1e14 = 1.1 мс`.<d-footnote>Обратите внимание, что эти чипы имеют разную цену, и данное сравнение не нормализовано по стоимости.</d-footnote>

**Обмен данными внутри чипа:** Тензоры нужно постоянно перемещать между памятью высокой пропускной способности (HBM) и вычислительными ядрами ускорителя. Скорость этого обмена называют "пропускной способностью HBM"<d-footnote>NVIDIA также использует термин "пропускная способность памяти"</d-footnote>. На H100 она составляет [около 3.35 ТБ/с](https://www.nvidia.com/en-us/data-center/h100/), на TPU v6e — [около 1.6 ТБ/с](https://cloud.google.com/tpu/docs/v6e).

**Обмен данными между чипами:** При распределении модели на несколько ускорителей тензоры приходится постоянно передавать между ними. Обычно для этого есть несколько вариантов связи (ICI, DCN и PCIe), каждый со своей пропускной способностью.

Независимо от того, идёт ли обмен данными внутри чипа или между чипами, мы измеряем его в байт/с и оцениваем общее время коммуникации по формуле:

$$\begin{equation}
T_\text{comms} = \frac{\text{Объём данных (байты)}}{\text{Пропускная способность (байт/с)}}
\end{equation}$$

Обычно (но не всегда) вычисления внутри одного чипа можно совместить с обменом данными как внутри чипа, так и между чипами. Это означает, что **нижнюю границу времени обучения и инференса можно оценить через максимум из времени вычислений и времени коммуникации**. **Верхнюю границу** можно оценить **их суммой**. На практике мы оптимизируем по максимуму, так как математика проще, и обычно можно приблизиться к этой границе, совмещая коммуникацию с вычислениями. При оптимизации по максимуму нижняя и верхняя границы различаются не более чем в 2 раза, поскольку $T_\text{math} + T_\text{comms} \leq 2 \cdot \max(T_\text{math}, T_\text{comms})$. Затем мы повышаем точность оценки, моделируя «области перекрытия» и накладные расходы — для этого можно использовать профилирование вашей конкретной модели и целевой системы.

$$\begin{equation}
T_\text{lower}=\max(T_\text{math}, T_\text{comms})
\end{equation}$$

$$\begin{equation}
T_\text{upper} = T_\text{math} + T_\text{comms}
\end{equation}$$

Если предположить, что мы можем идеально совместить коммуникацию и вычисления, то при $T_\text{math} > T_\text{comms}$ железо используется полностью. Это называется "compute-bound" (ограничение по вычислениям). Когда $T_\text{comms} > T_\text{math}$, мы "communication-bound" (ограничены коммуникацией), и часть FLOPS ускорителя простаивает в ожидании передачи данных. Определить, будет ли операция ограничена вычислениями или коммуникацией, можно по её "*arithmetic intensity*" (арифметической интенсивности) или "*operational intensity*" (операционной интенсивности).

**Определение:** арифметическая интенсивность алгоритма — это отношение общего числа операций (FLOPs) к объёму передаваемых данных в байтах (внутри чипа или между чипами).

$$\begin{equation}
\text{Арифметическая интенсивность} = \frac{\text{Вычислительная сложность (FLOPs)}}{\text{Объём данных (байты)}}
\end{equation}$$

Арифметическая интенсивность показывает количество операций на байт для данной операции. В первом приближении: когда интенсивность высокая, $T_\text{math}$ значительно больше $T_\text{comms}$, и мы используем большую часть доступных FLOPs. В обратном случае мы тратим больше времени на передачу данных и теряем вычислительную мощность. Точка, где происходит этот переход, называется "пиковой арифметической интенсивностью" железа — это отношение пиковой производительности ускорителя (FLOPs/s) к его пропускной способности (байт/с).

$$\begin{align*}
T_\text{math} > T_\text{comms} \Leftrightarrow \frac{\text{Вычислительная сложность (FLOPs)}} {\text{Производительность ускорителя (FLOPs/s)}} > \frac{\text{Объём данных (байты)}}{\text{Пропускная способность (байт/с)}} & \\[0.5em]
\Leftrightarrow \frac{\text{Вычислительная сложность (FLOPs)}}{\text{Объём данных (байты)}} > \frac{\text{Производительность ускорителя (FLOPs/s)}}{\text{Пропускная способность (байт/с)}} & \\[0.5em]
\Leftrightarrow \text{Интенсивность}(\text{Вычисления}) > \text{Интенсивность}(\text{Ускоритель}) & \\
\end{align*}$$

Величина $\text{Интенсивность}(\text{Ускоритель})$ — это арифметическая интенсивность, при которой ускоритель достигает пиковой производительности в FLOPs/s. **Для MXU в TPU v5e это примерно 240 FLOPs/байт**, поскольку TPU может выполнять `1.97e14` FLOPs/s и загружать `8.2e11` байт/с из HBM.<d-footnote>MXU (matrix multiply unit) — это блок матричного умножения в TPU. Мы уточняем это, потому что в TPU есть и другие ускорители, например VPU, отвечающий за поэлементные операции с другой пиковой производительностью.</d-footnote> Это означает, что если алгоритм имеет арифметическую интенсивность ниже 240 FLOPs/байт, он будет ограничен скоростью загрузки данных, и мы не сможем эффективно использовать железо.<d-footnote>Это справедливо только если алгоритм загружает веса из HBM и работает в MXU. Как мы обсудим в следующем разделе, иногда параметры можно хранить в VMEM, у которой значительно выше пропускная способность. Многие алгоритмы также работают в VPU с другими характеристиками производительности.</d-footnote> Рассмотрим один такой пример:

**<span style="color:#7ab5ff">Пример (скалярное произведение)</span>:** чтобы вычислить скалярное произведение двух векторов в точности bfloat16, `x • y: bf16[N], bf16[N] → bf16[1]`, нужно загрузить $x$ и $y$ из памяти (каждый по $2 * N = 2N$ байт), выполнить $N$ умножений и $N-1$ сложений, и записать $2$ байта обратно в HBM
$$\begin{equation}
\text{Интенсивность}(\text{скалярное произведение}) = \frac{\text{Всего FLOPs}}{\text{Всего байт}} = \frac{N + N - 1}{2N + 2N + 2} = \frac{2N - 1}{4N + 2} \rightarrow \frac{1}{2}
\end{equation}$$

при $N\rightarrow\infty$. Таким образом, скалярное произведение имеет арифметическую интенсивность $\frac{1}{2}$, или, другими словами, выполняет 0.5 операции с плавающей точкой на байт загруженных данных. Это означает, что наша арифметическая интенсивность ниже, чем у железа, и мы будем ограничены скоростью передачи данных.<d-footnote>Значение 240 выше не подходит для сравнения, поскольку, как мы увидим в следующем разделе, скалярное произведение выполняется на VPU, а не на MXU. VPU в TPU v5p может выполнять примерно 7e12 FLOPs/секунду, поэтому его критическая интенсивность около 3, что означает, что мы всё ещё частично ограничены передачей данных. В любом случае, низкая и константная интенсивность означает, что на большинстве железа сложно быть ограниченным вычислениями.</d-footnote>

### Визуализация roofline-графиков

Компромисс между памятью и вычислениями можно визуализировать с помощью **roofline-графика**, который отображает достижимую пиковую производительность (FLOPs/s) алгоритма на нашем железе (ось Y) в зависимости от арифметической интенсивности этого алгоритма (ось X). Вот пример в логарифмическом масштабе:

{% include figure.liquid path="assets/img/roofline-improved.png" class="img-fluid" caption="<b>Рисунок:</b> пример roofline-графика, показывающий два алгоритма с разной арифметической интенсивностью (Algo 1 и Algo 2) и их теоретическую пиковую производительность при разной пропускной способности (BW1 и BW2). В красной области алгоритм ограничен пропускной способностью при обеих BW и теряет часть пиковой производительности железа. Жёлтая область ограничена пропускной способностью только при низкой BW1. Зелёная область ограничена вычислениями при любой пропускной способности. Здесь мы используем пиковую производительность ускорителя, и увеличение пропускной способности или интенсивности не даёт выигрыша." %}

По мере роста интенсивности (слева направо) мы сначала видим линейный рост производительности алгоритма (в FLOPs/s), пока не достигнем критической арифметической интенсивности железа — 240 для TPU v5e. Любой алгоритм с меньшей интенсивностью будет ограничен пропускной способностью памяти (BW, показано красным). Любой алгоритм правее будет полностью использовать доступные FLOPs (показано зелёным). Здесь Algo 1 ограничен передачей данных и использует лишь часть общей производительности железа. Algo 2 ограничен вычислениями. В общем случае производительность алгоритма можно улучшить либо увеличив его арифметическую интенсивность, либо увеличив доступную пропускную способность памяти (переход от BW1 к BW2).

### Матричное умножение

Рассмотрим наш любимый алгоритм: матричное умножение (matmul). Запишем $X * Y \rightarrow Z$; где $X$ имеет размерность $\text{bf16}[B, D]$; $Y$ размерности $\text{bf16}[D, F]$; а $Z$ размерности $\text{bf16}[B, F]$. Для выполнения matmul нужно загрузить $2DF + 2BD$ байт, выполнить $2BDF$ FLOPs и записать обратно $2BF$ байт.<d-footnote>Технически мы выполняем $BF \times (2D - 1)$ FLOPs, но это достаточно близко. Это $BDF$ умножений и $BF * (D-1)$ сложений. Подробнее в разделе 4.</d-footnote> <d-footnote>Хотя результат matmul технически получается в float32, обычно мы приводим его к bfloat16 перед записью обратно в HBM.</d-footnote> Таким образом:

$$\begin{equation}
\text{Интенсивность}(\text{matmul}) = \frac{2BDF}{2BD + 2DF + 2BF} = \frac{BDF}{BD + DF + BF}
\end{equation}$$

Можно упростить, если предположить, что "размер батча" $B$ мал относительно $D$ и $F$. Тогда получим

$$\begin{equation}
\frac{BDF}{BD + DF + BF} \approxeq \frac{BDF}{DF} = B
\end{equation}$$

$$\begin{equation}
\text{Интенсивность}(\text{matmul}) > \text{Интенсивность}(\text{TPU}) \implies B > \frac{1.97e14}{8.20e11} = 240
\end{equation}$$

Это разумное предположение для matmul в трансформерах, поскольку обычно локальный (на реплику) размер батча $B < 1024$ токенов (*не* последовательностей), но $D$ и $F > 8000$. Таким образом, мы становимся ограничены вычислениями, когда размер батча на реплику<d-footnote>Мы говорим "на реплику", потому что при использовании шардинга модели для увеличения числа чипов в matmul мы масштабируем одновременно и доступные вычисления, и пропускную способность памяти. Поэтому критический размер батча справедлив для каждой независимой копии весов модели.</d-footnote> превышает 240 токенов — очень простое правило!

<p markdown=1 class="takeaway">**Вывод:** чтобы matmul в bfloat16 был ограничен вычислениями на большинстве TPU, размер батча в токенах на реплику должен превышать 240.<d-footnote>Обратите внимание, что это _не_ размер батча в обычном смысле (количество последовательностей). Оказывается, большинство roofline зависят только от числа токенов, независимо от того, принадлежат ли они одной или разным последовательностям. Например, если у вас батч из 512 последовательностей по 4096 токенов на 128 GPU, у вас общий батч `512 * 4096 = 2M` токенов и локальный батч 16k токенов.</d-footnote></p>

Есть несколько важных оговорок, которые мы рассмотрим в задачах ниже, особенно касающихся квантизации (например, когда мы квантизуем активации, но всё ещё выполняем FLOPs в полной точности), но это правило полезно запомнить. Для GPU это число чуть выше (ближе к 300), но общий вывод тот же. Когда мы [разбиваем большой matmul на меньшие](https://docs.jax.dev/en/latest/pallas/tpu/matmul.html#your-first-matrix-multiplication-kernel), размеры тайлов также имеют значение.<d-footnote>При большом матричном умножении нужно разбить его на меньшие тайлы, которые помещаются в VMEM/SMEM/TMEM — высокоскоростную память на чипе. Из-за этого мы загружаем блоки несколько раз, поэтому уже не совсем верно, что загружаем только $O(N^2)$ байт. Рассмотрим matmul $(m, k) \cdot (k, n)$ с размерами тайлов $bm$, $bk$, $bn$. Пусть $tm = m / bm$ и т.д. Тогда общие FLOPs — это $2 \cdot tm \cdot tn \cdot tk \cdot bm \cdot bn \cdot bk$, а общие байты — $2 \cdot tm \cdot tn \cdot (tk \cdot (bm \cdot bk + bk \cdot bn) + 2 \cdot bm \cdot bn)$. Игнорируя последний член, получаем интенсивность $bm \cdot bn / (bm + bn)$, что похоже на вышеприведённое.</d-footnote> Низкоуровневые детали GPU и TPU мы обсудим в [следующем разделе](../tpus).

### Roofline для сетевой коммуникации

Все roofline-графики, которые мы обсуждали до сих пор, относились к пропускной способности памяти _внутри одного чипа_. На самом деле, большинство roofline в этой книге связаны с коммуникацией между чипами: обычно это матричные умножения, где матрицы распределены (шардированы) между несколькими TPU.

Рассмотрим несколько искусственный пример: допустим, мы хотим перемножить две большие матрицы $X\sim \text{bfloat16[B, D]}$ и $Y \sim \text{bfloat16[D, F]}$, которые равномерно распределены между 2 TPU/GPU (по размерности $D$). Чтобы выполнить это умножение (как мы увидим в [разделе 3](../sharding)), можно перемножить половину каждой матрицы на каждом TPU (`A = X[:, :D // 2] @ Y[:D // 2, :]` на TPU 0 и `B = X[:, D // 2:] @ Y[D // 2:, :]` на TPU 1), а затем скопировать получившиеся "частичные суммы" на другой TPU и сложить их. Допустим, мы можем передать `4.5e10` байт в каждом направлении и выполнять `1.97e14` FLOPs/s на каждом чипе. Чему равны $T_\text{math}$ и $T_\text{comms}$?

$T_\text{math}$ очевидно вдвое меньше, чем раньше, поскольку каждый TPU выполняет половину работы, то есть<d-footnote>Мы игнорируем FLOPs, необходимые для сложения двух частичных сумм (ещё DF сложений), но это пренебрежимо мало.</d-footnote>

$$T_\text{math} = \frac{2BDF}{2 \cdot \text{Производительность ускорителя (FLOPs/s)}} = \frac{BDF}{1.97e14}$$

А что с $T_\text{comms}$? Теперь это время коммуникации между чипами! Это просто общий объём переданных данных, делённый на пропускную способность сети:

$$T_\text{comms} = \frac{2BF}{\text{Пропускная способность сети}} = \frac{2BF}{4.5e10}$$

Следовательно, мы становимся ограничены вычислениями (теперь относительно межчиповой сети), когда $$\text{Интенсивность}(\text{matmul (2 чипа)}) > \text{Интенсивность}(\text{TPU относительно межчиповой сети})$$ или, что эквивалентно, когда $\frac{BDF}{2BF} = \frac{D}{2} > \frac{1.97e14}{4.5e10} = 4377$, то есть $D > 8755$. Обратите внимание: в отличие от предыдущего случая, критический порог теперь зависит от $D$, а не от $B$! Попробуйте понять, почему. Это лишь один пример, но мы подчёркиваем, что такой roofline критически важен для понимания, когда можно распараллелить операцию между несколькими TPU.

## Несколько задач для практики

**Задача 1 [int8 matmul]:** Допустим, мы хотим выполнить matmul $X[B, D] \cdot_D Y[D, F] \rightarrow Z[B, F]$ в точности int8 (1 байт на параметр) вместо bfloat16.<d-footnote>Здесь и далее мы используем нотацию $A \cdot_D B$ для обозначения умножения со сверткой по размерности D. Это вольность в использовании einsum-нотации.</d-footnote>

1. Сколько байт нужно загрузить из памяти? Сколько нужно записать обратно в память?
2. Сколько всего операций (OPs) выполняется?
3. Какова арифметическая интенсивность?
4. Какова roofline-оценка для $T_\text{math}$ и $T_\text{comms}$? Каковы разумные верхняя и нижняя границы времени выполнения всей операции?

Предположим, что пропускная способность HBM составляет `8.1e11` байт/с, а пиковая производительность int8 — `3.94e14` OPs/s (примерно в 2 раза выше bfloat16).

{% details Нажмите, чтобы увидеть ответ. %}

1. Поскольку параметры хранятся в int8, у нас 1 байт на параметр, поэтому из HBM загружается $$BD + DF$$ байт и записывается обратно $$BF$$ байт.
2. Это то же самое, что и в bfloat16, но теоретически int8 OPs/s должен быть быстрее. Итого всё ещё $2BDF$ FLOPs.
3. Арифметическая интенсивность составляет $$2BDF / (BD + DF + BF)$$. Если сделать то же предположение, что $$B \ll D$$ и $$B \ll F$$, получим арифметическую интенсивность $$2B$$, то есть наше правило принимает вид $B > \text{арифметическая интенсивность HBM для int8} / 2$. Используя приведённые числа, эта int8-интенсивность равна `3.94e14 / 8.1e11 = 486`, поэтому правило имеет вид $B > 486 / 2 = 243$. Обратите внимание, что это практически не изменилось!
4. $$T_\text{math} = 2BDF / 3.94e14$$ и $$T_\text{comms} = (BD + DF + BF) / 8.1e11$$, поэтому разумная нижняя граница — это $$\max(T_\text{math}, T_\text{comms})$$, а верхняя граница — $$T_\text{math} + T_\text{comms}$$.

{% enddetails %}

**Задача 2 [int8 + bf16 matmul]:** На практике мы часто используем разную квантизацию для весов и активаций: веса можем хранить в очень низкой точности, а активации (и вычисления) — в более высокой. Допустим, мы хотим квантизовать веса в int8, но сохранить активации (и вычисления) в bfloat16. При каком размере батча мы становимся ограничены вычислениями? Предположим производительность `1.97e14` bfloat16 FLOPs/s.

*Подсказка: это означает конкретно `bfloat16[B, D] * int8[D, F] -> bfloat16[B, F]`, где $B$ — это "размер батча".*

{% details Нажмите, чтобы увидеть ответ. %}

Снова предполагая, что B мало, имеем 2BDF bfloat16 FLOPs, но только DF весов (вместо 2DF в bfloat16). Это означает, что мы становимся ограничены вычислениями, когда $$2B > 240$$ или $$B > 120$$. Это намного ниже, что означает: если мы можем квантизовать веса в int8 (что довольно легко сделать), но всё ещё выполнять FLOPs в bfloat16, мы получаем ощутимый выигрыш в эффективности (хотя int8 OPs были бы ещё лучше).

{% enddetails %}

**Задача 3:** Используя условия из задачи 2, постройте roofline-график пиковой производительности (FLOPs/s) в зависимости от $B$ для $F = D = 4096$ и $F = D = 1024$. *Используйте точное количество загружаемых байт, а не приближение.*

{% details Нажмите, чтобы увидеть ответ. %}

Вот требуемый график:

{% include figure.liquid path="assets/img/roofline-plot-q3.png" class="img-fluid img-small" %}

Обратите внимание, что обе модели в конечном итоге достигают пиковой производительности железа, но при больших D/F это происходит раньше. При D=F=1024 критический размер батча почти удваивается. Код для генерации этого графика:

```py
import matplotlib.pyplot as plt
import numpy as np

bs = np.arange(1, 512)

def roofline(B, D, F):
  total_flops = 2*B*D*F
  flops_time = total_flops / 1.97e14
  comms_time = (2*B*D + D*F + 2*B*F) / 8.2e11
  total_time = np.maximum(flops_time, comms_time)
  return total_flops / total_time

roofline_big = roofline(bs, 4096, 4096)
roofline_small = roofline(bs, 1024, 1024)

plt.figure(figsize=(8, 4))
plt.plot(bs, roofline_big, label='F=D=4096')
plt.plot(bs, roofline_small, label='F=D=1024')
plt.legend()
plt.xlabel('batch size')
plt.ylabel('peak bfloat16 FLOPs/s on TPU v5e')
plt.grid()
```

{% enddetails %}

**Задача 4:** Что если мы хотим выполнить $\text{int8[B, D]} *_D \text{int8[B, D, F]} \rightarrow \text{int8[B, F]}$, где для каждого элемента батча используется своя матрица. Какова арифметическая интенсивность этой операции?

{% details Нажмите, чтобы увидеть ответ. %}

Начнём с общих FLOPs и коммуникаций.

1. Всего FLOPs: количество FLOPs в основном то же, поскольку мы выполняем то же количество matmul размером $$BD \times DF$$ (подробнее в разделе 4). Итого $$2BDF$$.
2. Всего коммуникаций: здесь коммуникаций значительно больше: $$BD + BDF + BF$$.
3. Следовательно, наша арифметическая интенсивность теперь равна $$2BDF / (BD + BDF + BF)$$. Поскольку $$BDF$$ доминирует в знаменателе, это примерно $$2$$. То есть вместо зависимости от размера батча интенсивность фактически константна. Это плохо, потому что означает, что мы практически всегда будем ограничены коммуникациями, независимо от параметров.

{% enddetails %}

**Задача 5 [Rooflines для GPU]:** Используя [спецификацию NVIDIA для H100](https://www.nvidia.com/en-us/data-center/h100/), рассчитайте размер батча, при котором матричное умножение становится ограниченным вычислениями. *Обратите внимание, что значения FLOPs для Tensor Core указаны в два раза больше реального значения, поскольку они достижимы только при структурированной разреженности.*

{% details Нажмите, чтобы увидеть ответ. %}

Из спецификации видно, что заявленное значение bfloat16 FLOPs составляет `1.979e15` FLOPs/s со звёздочкой "с разреженностью". Реальное значение без разреженности вдвое меньше, то есть около `1e15` FLOPs/s. Пропускная способность памяти — 3.35TB/s, или `3.35e12` байт/секунду. Таким образом, $B_\text{crit}$ равно `1e15 / 3.35e12 = 298`, что довольно близко к TPU.

{% enddetails %}

<h3 markdown=1 class="next-section">На этом часть 1 завершена! Для части 2, где рассматривается, как реальные TPU производят вычисления, [нажмите здесь](../tpus).</h3>
