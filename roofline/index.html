<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Все о Rooflines | How To Scale Your Model </title> <meta name="author" content=" "> <meta name="description" content="Когда мы запускаем алгоритмы на железе, нас ограничивают три вещи: скорость вычислений (операций/сек), пропускная способность для перемещения данных (байт/сек) и общий объём доступной памяти для хранения данных (байты). Эти ограничения (roofline) позволяют нам оценить верхнюю и нижнюю границы времени выполнения конкретных вычислений. "> <meta name="keywords" content="scaling, jax, llms, transformers, tpus, google, deepmind, parallelism, pallas"> <link rel="stylesheet" href="/scaling-book/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/scaling-book/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/scaling-book/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/scaling-book/assets/css/jekyll-pygments-themes-vs.css?4ee1a2facd1a8a76347f4bd43a740500" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/scaling-book/assets/img/favicon.ico?fddbd8c2ec231ba2060e67c85de32a55"> <link rel="stylesheet" href="/scaling-book/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kineticcookie.github.io/scaling-book/roofline/"> <script src="/scaling-book/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/scaling-book/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/scaling-book/assets/js/distillpub/template.v2.js"></script> <script src="/scaling-book/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">{{page._styles}}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Все о Rooflines",
            "description": "Когда мы запускаем алгоритмы на железе, нас ограничивают три вещи: скорость вычислений (операций/сек), пропускная способность для перемещения данных (байт/сек) и общий объём доступной памяти для хранения данных (байты). Эти ограничения (roofline) позволяют нам оценить верхнюю и нижнюю границы времени выполнения конкретных вычислений. ",
            "published": "February 04, 2025",
            "authors": [
              
              {
                "author": "Jacob Austin",
                "authorURL": "https://www.jacobaustin.org/",
                "affiliations": [
                  {
                    "name": "Google DeepMind",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sholto Douglas",
                "authorURL": "https://x.com/_sholtodouglas",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Roy Frostig",
                "authorURL": "https://cs.stanford.edu/~rfrostig/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Anselm Levskaya",
                "authorURL": "https://anselmlevskaya.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Charlie Chen",
                "authorURL": "https://x.com/charliexychen",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sharad Vikram",
                "authorURL": "https://sharadvikram.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Federico Lebron",
                "authorURL": "https://fedelebron.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Peter Choy",
                "authorURL": "https://x.com/pchoy95",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Vinay Ramasesh",
                "authorURL": "https://x.com/vinayramasesh",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Albert Webson",
                "authorURL": "https://representation.ai/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Reiner Pope<sup>*</sup>",
                "authorURL": "https://x.com/reinerpope",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <script>
    function goToTop() {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    }

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
      // Get the button:
      let mybutton = document.getElementById("top-button");

      if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
  }
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/scaling-book"> How To Scale Your Model </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="left-button section-button"><a href=".."><svg viewbox="-78.5 0 512 512"><path d="M257 64L291 98 128 262 291 426 257 460 61 262 257 64Z"></path></svg></a></div> <div class="right-button section-button"><a href="../tpus"><svg viewbox="-78.5 0 512 512"><path d="M98 460L64 426 227 262 64 98 98 64 294 262 98 460Z"></path></svg></a></div> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/scaling-book/"> </a> </li> <li class="nav-item nav-hidden"><a class="nav-link" onclick="goToTop()" id="top-button" style="display: none;">Back to Top</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item nav-hidden"><a class="nav-link" href="..">Previous Part</a></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../tpus">Next Part</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Sections </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/scaling-book/index">Part 0. Introduction</a> <a class="dropdown-item " href="/scaling-book/roofline">Part 1. Intro to Rooflines</a> <a class="dropdown-item " href="/scaling-book/tpus">Part 2. All About TPUs</a> <a class="dropdown-item " href="/scaling-book/sharding">Part 3. Sharded Matmuls</a> <a class="dropdown-item " href="/scaling-book/transformers">Part 4. Transformers</a> <a class="dropdown-item " href="/scaling-book/training">Part 5. Training</a> <a class="dropdown-item " href="/scaling-book/applied-training">Part 6. Training LLaMA</a> <a class="dropdown-item " href="/scaling-book/inference">Part 7. Inference</a> <a class="dropdown-item " href="/scaling-book/applied-inference">Part 8. Serving LLaMA</a> <a class="dropdown-item " href="/scaling-book/profiling">Part 9. Profiling</a> <a class="dropdown-item " href="/scaling-book/jax-stuff">Part 10. All About JAX</a> <a class="dropdown-item " href="/scaling-book/conclusion">Part 11. Conclusions</a> <a class="dropdown-item " href="/scaling-book/gpus">Part 12. GPUs</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Все о Rooflines</h1> <p>Part 1 of <a href="/scaling-book">How To Scale Your Model</a> (<a href="..">Часть 0: Введение</a> | <a href="../tpus">Часть 2: TPU</a>)</p> <p>Когда мы запускаем алгоритмы на железе, нас ограничивают три вещи: скорость вычислений (операций/сек), пропускная способность для перемещения данных (байт/сек) и общий объём доступной памяти для хранения данных (байты). Эти ограничения (roofline) позволяют нам оценить верхнюю и нижнюю границы времени выполнения конкретных вычислений. </p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Содержание</h3> <div> <a href="#%D0%BA%D1%83%D0%B4%D0%B0-%D1%83%D1%85%D0%BE%D0%B4%D0%B8%D1%82-%D0%B2%D1%80%D0%B5%D0%BC%D1%8F">Куда уходит время?</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#%D0%B2%D0%B8%D0%B7%D1%83%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-roofline-%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D0%BA%D0%BE%D0%B2">Визуализация roofline-графиков</a> </li> <li> <a href="#%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%87%D0%BD%D0%BE%D0%B5-%D1%83%D0%BC%D0%BD%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5">Матричное умножение</a> </li> <li> <a href="#roofline-%D0%B4%D0%BB%D1%8F-%D1%81%D0%B5%D1%82%D0%B5%D0%B2%D0%BE%D0%B9-%D0%BA%D0%BE%D0%BC%D0%BC%D1%83%D0%BD%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8">Roofline для сетевой коммуникации</a> </li> </ul> <div> <a href="#%D0%BD%D0%B5%D1%81%D0%BA%D0%BE%D0%BB%D1%8C%D0%BA%D0%BE-%D0%B7%D0%B0%D0%B4%D0%B0%D1%87-%D0%B4%D0%BB%D1%8F-%D0%BF%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D0%B8">Несколько задач для практики</a> </div> </nav> </d-contents> <h2 id="куда-уходит-время">Куда уходит время?</h2> <p>Начнём с очень простого вопроса: <em>почему алгоритм выполняется 50 миллисекунд, а не 50 секунд или 5 миллисекунд?</em> Что именно происходит внутри модели, что занимает столько времени, и сколько это вообще должно занимать?</p> <blockquote> <p><strong>От переводчика:</strong> Возможна путаница связанная с FLOPs. В оригинале используется термин “FLOPs”, который является множественным числом от “FLOP” (floating point operation). То есть если у алгоритма 5 операций сложения с плавающей точкой, то у него 5 FLOPs. Проблема в том что в интернете часто используют “FLOPS” (floating point operations per second) для обозначения количества операций в секунду, когда говорят о производительности железа. С непривычки это может сбивать с толку потому что в книге производительность пишут через FLOPs/s.</p> <p>То есть в нотации этой книги:</p> <ul> <li> <strong>FLOPs</strong> — <strong>количество</strong> операций с плавающей точкой (например, сложность алгоритма – 1e12 FLOPs)</li> <li> <strong>FLOPs/s</strong> — количество операций с плавающей точкой <strong>в секунду</strong> (например, производительность NVIDIA GeForce RTX 4070 Ti SUPER – 44.10e12 FLOPs/s)</li> <li>FLOPS - не используется в этой книге, чтобы избежать путаницы. Вместо этого будет использоваться FLOPs/s.</li> </ul> </blockquote> <p><strong>Вычисления:</strong> По сути, модель глубокого обучения — это куча матричных умножений, каждое из которых состоит из операций умножения и сложения с плавающей точкой (FLOPs). Скорость нашего ускорителя определяет, сколько времени займут эти вычисления:</p> \[\begin{equation} T_\text{math} = \frac{\text{Вычислительная сложность (FLOPs)}}{\text{Производительность ускорителя (FLOPs/s)}} \end{equation}\] <p>Например, NVIDIA H100 выполняет примерно 9.89e14 операций bfloat16<d-footnote>bf16 — это сокращение от <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format" rel="external nofollow noopener" target="_blank">bfloat16</a>, 16-битного формата с плавающей точкой, который часто используется в ML.</d-footnote> в секунду, а TPU v6e — 9.1e14 FLOPs/s.<d-footnote>H100 и B200 обычно достигают лишь 80-85% от заявленной пиковой производительности, в то время как TPU при обычном использовании могут выйти на 95%.</d-footnote> Это значит, что выполнение 1e12 FLOPs на H100 займёт примерно <code class="language-plaintext highlighter-rouge">1e12 / 9.89e14 = 1.01 мс</code>, а на TPU v6e — <code class="language-plaintext highlighter-rouge">1e12 / 9.1e14 = 1.1 мс</code>.<d-footnote>Обратите внимание, что эти чипы имеют разную цену, и данное сравнение не нормализовано по стоимости.</d-footnote></p> <p><strong>Обмен данными внутри чипа:</strong> Тензоры нужно постоянно перемещать между памятью высокой пропускной способности (HBM) и вычислительными ядрами ускорителя. Скорость этого обмена называют “пропускной способностью HBM”<d-footnote>NVIDIA также использует термин "пропускная способность памяти"</d-footnote>. На H100 она составляет <a href="https://www.nvidia.com/en-us/data-center/h100/" rel="external nofollow noopener" target="_blank">около 3.35 ТБ/с</a>, на TPU v6e — <a href="https://cloud.google.com/tpu/docs/v6e" rel="external nofollow noopener" target="_blank">около 1.6 ТБ/с</a>.</p> <p><strong>Обмен данными между чипами:</strong> При распределении модели на несколько ускорителей тензоры приходится постоянно передавать между ними. Обычно для этого есть несколько вариантов связи (ICI, DCN и PCIe), каждый со своей пропускной способностью.</p> <p>Независимо от того, идёт ли обмен данными внутри чипа или между чипами, мы измеряем его в байт/с и оцениваем общее время коммуникации по формуле:</p> \[\begin{equation} T_\text{comms} = \frac{\text{Объём данных (байты)}}{\text{Пропускная способность (байт/с)}} \end{equation}\] <p>Обычно (но не всегда) вычисления внутри одного чипа можно совместить с обменом данными как внутри чипа, так и между чипами. Это означает, что <strong>нижнюю границу времени обучения и инференса можно оценить через максимум из времени вычислений и времени коммуникации</strong>. <strong>Верхнюю границу</strong> можно оценить <strong>их суммой</strong>. На практике мы оптимизируем по максимуму, так как математика проще, и обычно можно приблизиться к этой границе, совмещая коммуникацию с вычислениями. При оптимизации по максимуму нижняя и верхняя границы различаются не более чем в 2 раза, поскольку $T_\text{math} + T_\text{comms} \leq 2 \cdot \max(T_\text{math}, T_\text{comms})$. Затем мы повышаем точность оценки, моделируя «области перекрытия» и накладные расходы — для этого можно использовать профилирование вашей конкретной модели и целевой системы.</p> \[\begin{equation} T_\text{lower}=\max(T_\text{math}, T_\text{comms}) \end{equation}\] \[\begin{equation} T_\text{upper} = T_\text{math} + T_\text{comms} \end{equation}\] <p>Если предположить, что мы можем идеально совместить коммуникацию и вычисления, то при $T_\text{math} &gt; T_\text{comms}$ железо используется полностью. Это называется “compute-bound” (ограничение по вычислениям). Когда $T_\text{comms} &gt; T_\text{math}$, мы “communication-bound” (ограничены коммуникацией), и часть FLOPS ускорителя простаивает в ожидании передачи данных. Определить, будет ли операция ограничена вычислениями или коммуникацией, можно по её “<em>arithmetic intensity</em>” (арифметической интенсивности) или “<em>operational intensity</em>” (операционной интенсивности).</p> <p><strong>Определение:</strong> арифметическая интенсивность алгоритма — это отношение общего числа операций (FLOPs) к объёму передаваемых данных в байтах (внутри чипа или между чипами).</p> \[\begin{equation} \text{Арифметическая интенсивность} = \frac{\text{Вычислительная сложность (FLOPs)}}{\text{Объём данных (байты)}} \end{equation}\] <p>Арифметическая интенсивность показывает количество операций на байт для данной операции. В первом приближении: когда интенсивность высокая, $T_\text{math}$ значительно больше $T_\text{comms}$, и мы используем большую часть доступных FLOPs. В обратном случае мы тратим больше времени на передачу данных и теряем вычислительную мощность. Точка, где происходит этот переход, называется “пиковой арифметической интенсивностью” железа — это отношение пиковой производительности ускорителя (FLOPs/s) к его пропускной способности (байт/с).</p> \[\begin{align*} T_\text{math} &gt; T_\text{comms} \Leftrightarrow \frac{\text{Вычислительная сложность (FLOPs)}} {\text{Производительность ускорителя (FLOPs/s)}} &gt; \frac{\text{Объём данных (байты)}}{\text{Пропускная способность (байт/с)}} &amp; \\[0.5em] \Leftrightarrow \frac{\text{Вычислительная сложность (FLOPs)}}{\text{Объём данных (байты)}} &gt; \frac{\text{Производительность ускорителя (FLOPs/s)}}{\text{Пропускная способность (байт/с)}} &amp; \\[0.5em] \Leftrightarrow \text{Интенсивность}(\text{Вычисления}) &gt; \text{Интенсивность}(\text{Ускоритель}) &amp; \\ \end{align*}\] <p>Величина $\text{Интенсивность}(\text{Ускоритель})$ — это арифметическая интенсивность, при которой ускоритель достигает пиковой производительности в FLOPs/s. <strong>Для MXU в TPU v5e это примерно 240 FLOPs/байт</strong>, поскольку TPU может выполнять <code class="language-plaintext highlighter-rouge">1.97e14</code> FLOPs/s и загружать <code class="language-plaintext highlighter-rouge">8.2e11</code> байт/с из HBM.<d-footnote>MXU (matrix multiply unit) — это блок матричного умножения в TPU. Мы уточняем это, потому что в TPU есть и другие ускорители, например VPU, отвечающий за поэлементные операции с другой пиковой производительностью.</d-footnote> Это означает, что если алгоритм имеет арифметическую интенсивность ниже 240 FLOPs/байт, он будет ограничен скоростью загрузки данных, и мы не сможем эффективно использовать железо.<d-footnote>Это справедливо только если алгоритм загружает веса из HBM и работает в MXU. Как мы обсудим в следующем разделе, иногда параметры можно хранить в VMEM, у которой значительно выше пропускная способность. Многие алгоритмы также работают в VPU с другими характеристиками производительности.</d-footnote> Рассмотрим один такой пример:</p> <p><strong><span style="color:#7ab5ff">Пример (скалярное произведение)</span>:</strong> чтобы вычислить скалярное произведение двух векторов в точности bfloat16, <code class="language-plaintext highlighter-rouge">x • y: bf16[N], bf16[N] → bf16[1]</code>, нужно загрузить $x$ и $y$ из памяти (каждый по $2 * N = 2N$ байт), выполнить $N$ умножений и $N-1$ сложений, и записать $2$ байта обратно в HBM \(\begin{equation} \text{Интенсивность}(\text{скалярное произведение}) = \frac{\text{Всего FLOPs}}{\text{Всего байт}} = \frac{N + N - 1}{2N + 2N + 2} = \frac{2N - 1}{4N + 2} \rightarrow \frac{1}{2} \end{equation}\)</p> <p>при $N\rightarrow\infty$. Таким образом, скалярное произведение имеет арифметическую интенсивность $\frac{1}{2}$, или, другими словами, выполняет 0.5 операции с плавающей точкой на байт загруженных данных. Это означает, что наша арифметическая интенсивность ниже, чем у железа, и мы будем ограничены скоростью передачи данных.<d-footnote>Значение 240 выше не подходит для сравнения, поскольку, как мы увидим в следующем разделе, скалярное произведение выполняется на VPU, а не на MXU. VPU в TPU v5p может выполнять примерно 7e12 FLOPs/секунду, поэтому его критическая интенсивность около 3, что означает, что мы всё ещё частично ограничены передачей данных. В любом случае, низкая и константная интенсивность означает, что на большинстве железа сложно быть ограниченным вычислениями.</d-footnote></p> <h3 id="визуализация-roofline-графиков">Визуализация roofline-графиков</h3> <p>Компромисс между памятью и вычислениями можно визуализировать с помощью <strong>roofline-графика</strong>, который отображает достижимую пиковую производительность (FLOPs/s) алгоритма на нашем железе (ось Y) в зависимости от арифметической интенсивности этого алгоритма (ось X). Вот пример в логарифмическом масштабе:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/roofline-improved-480.webp 480w,/scaling-book/assets/img/roofline-improved-800.webp 800w,/scaling-book/assets/img/roofline-improved-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/roofline-improved.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Рисунок:</b> пример roofline-графика, показывающий два алгоритма с разной арифметической интенсивностью (Algo 1 и Algo 2) и их теоретическую пиковую производительность при разной пропускной способности (BW1 и BW2). В красной области алгоритм ограничен пропускной способностью при обеих BW и теряет часть пиковой производительности железа. Жёлтая область ограничена пропускной способностью только при низкой BW1. Зелёная область ограничена вычислениями при любой пропускной способности. Здесь мы используем пиковую производительность ускорителя, и увеличение пропускной способности или интенсивности не даёт выигрыша.</figcaption> </figure> <p>По мере роста интенсивности (слева направо) мы сначала видим линейный рост производительности алгоритма (в FLOPs/s), пока не достигнем критической арифметической интенсивности железа — 240 для TPU v5e. Любой алгоритм с меньшей интенсивностью будет ограничен пропускной способностью памяти (BW, показано красным). Любой алгоритм правее будет полностью использовать доступные FLOPs (показано зелёным). Здесь Algo 1 ограничен передачей данных и использует лишь часть общей производительности железа. Algo 2 ограничен вычислениями. В общем случае производительность алгоритма можно улучшить либо увеличив его арифметическую интенсивность, либо увеличив доступную пропускную способность памяти (переход от BW1 к BW2).</p> <h3 id="матричное-умножение">Матричное умножение</h3> <p>Рассмотрим наш любимый алгоритм: матричное умножение (matmul). Запишем $X * Y \rightarrow Z$; где $X$ имеет размерность $\text{bf16}[B, D]$; $Y$ размерности $\text{bf16}[D, F]$; а $Z$ размерности $\text{bf16}[B, F]$. Для выполнения matmul нужно загрузить $2DF + 2BD$ байт, выполнить $2BDF$ FLOPs и записать обратно $2BF$ байт.<d-footnote>Технически мы выполняем $BF \times (2D - 1)$ FLOPs, но это достаточно близко. Это $BDF$ умножений и $BF * (D-1)$ сложений. Подробнее в разделе 4.</d-footnote> <d-footnote>Хотя результат matmul технически получается в float32, обычно мы приводим его к bfloat16 перед записью обратно в HBM.</d-footnote> Таким образом:</p> \[\begin{equation} \text{Интенсивность}(\text{matmul}) = \frac{2BDF}{2BD + 2DF + 2BF} = \frac{BDF}{BD + DF + BF} \end{equation}\] <p>Можно упростить, если предположить, что “размер батча” $B$ мал относительно $D$ и $F$. Тогда получим</p> \[\begin{equation} \frac{BDF}{BD + DF + BF} \approxeq \frac{BDF}{DF} = B \end{equation}\] \[\begin{equation} \text{Интенсивность}(\text{matmul}) &gt; \text{Интенсивность}(\text{TPU}) \implies B &gt; \frac{1.97e14}{8.20e11} = 240 \end{equation}\] <p>Это разумное предположение для matmul в трансформерах, поскольку обычно локальный (на реплику) размер батча $B &lt; 1024$ токенов (<em>не</em> последовательностей), но $D$ и $F &gt; 8000$. Таким образом, мы становимся ограничены вычислениями, когда размер батча на реплику<d-footnote>Мы говорим "на реплику", потому что при использовании шардинга модели для увеличения числа чипов в matmul мы масштабируем одновременно и доступные вычисления, и пропускную способность памяти. Поэтому критический размер батча справедлив для каждой независимой копии весов модели.</d-footnote> превышает 240 токенов — очень простое правило!</p> <p class="takeaway"><strong>Вывод:</strong> чтобы matmul в bfloat16 был ограничен вычислениями на большинстве TPU, размер батча в токенах на реплику должен превышать 240.<d-footnote>Обратите внимание, что это _не_ размер батча в обычном смысле (количество последовательностей). Оказывается, большинство roofline зависят только от числа токенов, независимо от того, принадлежат ли они одной или разным последовательностям. Например, если у вас батч из 512 последовательностей по 4096 токенов на 128 GPU, у вас общий батч `512 * 4096 = 2M` токенов и локальный батч 16k токенов.</d-footnote></p> <p>Есть несколько важных оговорок, которые мы рассмотрим в задачах ниже, особенно касающихся квантизации (например, когда мы квантизуем активации, но всё ещё выполняем FLOPs в полной точности), но это правило полезно запомнить. Для GPU это число чуть выше (ближе к 300), но общий вывод тот же. Когда мы <a href="https://docs.jax.dev/en/latest/pallas/tpu/matmul.html#your-first-matrix-multiplication-kernel" rel="external nofollow noopener" target="_blank">разбиваем большой matmul на меньшие</a>, размеры тайлов также имеют значение.<d-footnote>При большом матричном умножении нужно разбить его на меньшие тайлы, которые помещаются в VMEM/SMEM/TMEM — высокоскоростную память на чипе. Из-за этого мы загружаем блоки несколько раз, поэтому уже не совсем верно, что загружаем только $O(N^2)$ байт. Рассмотрим matmul $(m, k) \cdot (k, n)$ с размерами тайлов $bm$, $bk$, $bn$. Пусть $tm = m / bm$ и т.д. Тогда общие FLOPs — это $2 \cdot tm \cdot tn \cdot tk \cdot bm \cdot bn \cdot bk$, а общие байты — $2 \cdot tm \cdot tn \cdot (tk \cdot (bm \cdot bk + bk \cdot bn) + 2 \cdot bm \cdot bn)$. Игнорируя последний член, получаем интенсивность $bm \cdot bn / (bm + bn)$, что похоже на вышеприведённое.</d-footnote> Низкоуровневые детали GPU и TPU мы обсудим в <a href="../tpus">следующем разделе</a>.</p> <h3 id="roofline-для-сетевой-коммуникации">Roofline для сетевой коммуникации</h3> <p>Все roofline-графики, которые мы обсуждали до сих пор, относились к пропускной способности памяти <em>внутри одного чипа</em>. На самом деле, большинство roofline в этой книге связаны с коммуникацией между чипами: обычно это матричные умножения, где матрицы распределены (шардированы) между несколькими TPU.</p> <p>Рассмотрим несколько искусственный пример: допустим, мы хотим перемножить две большие матрицы $X\sim \text{bfloat16[B, D]}$ и $Y \sim \text{bfloat16[D, F]}$, которые равномерно распределены между 2 TPU/GPU (по размерности $D$). Чтобы выполнить это умножение (как мы увидим в <a href="../sharding">разделе 3</a>), можно перемножить половину каждой матрицы на каждом TPU (<code class="language-plaintext highlighter-rouge">A = X[:, :D // 2] @ Y[:D // 2, :]</code> на TPU 0 и <code class="language-plaintext highlighter-rouge">B = X[:, D // 2:] @ Y[D // 2:, :]</code> на TPU 1), а затем скопировать получившиеся “частичные суммы” на другой TPU и сложить их. Допустим, мы можем передать <code class="language-plaintext highlighter-rouge">4.5e10</code> байт в каждом направлении и выполнять <code class="language-plaintext highlighter-rouge">1.97e14</code> FLOPs/s на каждом чипе. Чему равны $T_\text{math}$ и $T_\text{comms}$?</p> <p>$T_\text{math}$ очевидно вдвое меньше, чем раньше, поскольку каждый TPU выполняет половину работы, то есть<d-footnote>Мы игнорируем FLOPs, необходимые для сложения двух частичных сумм (ещё DF сложений), но это пренебрежимо мало.</d-footnote></p> \[T_\text{math} = \frac{2BDF}{2 \cdot \text{Производительность ускорителя (FLOPs/s)}} = \frac{BDF}{1.97e14}\] <p>А что с $T_\text{comms}$? Теперь это время коммуникации между чипами! Это просто общий объём переданных данных, делённый на пропускную способность сети:</p> \[T_\text{comms} = \frac{2BF}{\text{Пропускная способность сети}} = \frac{2BF}{4.5e10}\] <p>Следовательно, мы становимся ограничены вычислениями (теперь относительно межчиповой сети), когда \(\text{Интенсивность}(\text{matmul (2 чипа)}) &gt; \text{Интенсивность}(\text{TPU относительно межчиповой сети})\) или, что эквивалентно, когда $\frac{BDF}{2BF} = \frac{D}{2} &gt; \frac{1.97e14}{4.5e10} = 4377$, то есть $D &gt; 8755$. Обратите внимание: в отличие от предыдущего случая, критический порог теперь зависит от $D$, а не от $B$! Попробуйте понять, почему. Это лишь один пример, но мы подчёркиваем, что такой roofline критически важен для понимания, когда можно распараллелить операцию между несколькими TPU.</p> <h2 id="несколько-задач-для-практики">Несколько задач для практики</h2> <p><strong>Задача 1 [int8 matmul]:</strong> Допустим, мы хотим выполнить matmul $X[B, D] \cdot_D Y[D, F] \rightarrow Z[B, F]$ в точности int8 (1 байт на параметр) вместо bfloat16.<d-footnote>Здесь и далее мы используем нотацию $A \cdot_D B$ для обозначения умножения со сверткой по размерности D. Это вольность в использовании einsum-нотации.</d-footnote></p> <ol> <li>Сколько байт нужно загрузить из памяти? Сколько нужно записать обратно в память?</li> <li>Сколько всего операций (OPs) выполняется?</li> <li>Какова арифметическая интенсивность?</li> <li>Какова roofline-оценка для $T_\text{math}$ и $T_\text{comms}$? Каковы разумные верхняя и нижняя границы времени выполнения всей операции?</li> </ol> <p>Предположим, что пропускная способность HBM составляет <code class="language-plaintext highlighter-rouge">8.1e11</code> байт/с, а пиковая производительность int8 — <code class="language-plaintext highlighter-rouge">3.94e14</code> OPs/s (примерно в 2 раза выше bfloat16).</p> <details><summary>Нажмите, чтобы увидеть ответ.</summary> <ol> <li>Поскольку параметры хранятся в int8, у нас 1 байт на параметр, поэтому из HBM загружается \(BD + DF\) байт и записывается обратно \(BF\) байт.</li> <li>Это то же самое, что и в bfloat16, но теоретически int8 OPs/s должен быть быстрее. Итого всё ещё $2BDF$ FLOPs.</li> <li>Арифметическая интенсивность составляет \(2BDF / (BD + DF + BF)\). Если сделать то же предположение, что \(B \ll D\) и \(B \ll F\), получим арифметическую интенсивность \(2B\), то есть наше правило принимает вид $B &gt; \text{арифметическая интенсивность HBM для int8} / 2$. Используя приведённые числа, эта int8-интенсивность равна <code class="language-plaintext highlighter-rouge">3.94e14 / 8.1e11 = 486</code>, поэтому правило имеет вид $B &gt; 486 / 2 = 243$. Обратите внимание, что это практически не изменилось!</li> <li>\(T_\text{math} = 2BDF / 3.94e14\) и \(T_\text{comms} = (BD + DF + BF) / 8.1e11\), поэтому разумная нижняя граница — это \(\max(T_\text{math}, T_\text{comms})\), а верхняя граница — \(T_\text{math} + T_\text{comms}\).</li> </ol> </details> <p><strong>Задача 2 [int8 + bf16 matmul]:</strong> На практике мы часто используем разную квантизацию для весов и активаций: веса можем хранить в очень низкой точности, а активации (и вычисления) — в более высокой. Допустим, мы хотим квантизовать веса в int8, но сохранить активации (и вычисления) в bfloat16. При каком размере батча мы становимся ограничены вычислениями? Предположим производительность <code class="language-plaintext highlighter-rouge">1.97e14</code> bfloat16 FLOPs/s.</p> <p><em>Подсказка: это означает конкретно <code class="language-plaintext highlighter-rouge">bfloat16[B, D] * int8[D, F] -&gt; bfloat16[B, F]</code>, где $B$ — это “размер батча”.</em></p> <details><summary>Нажмите, чтобы увидеть ответ.</summary> <p>Снова предполагая, что B мало, имеем 2BDF bfloat16 FLOPs, но только DF весов (вместо 2DF в bfloat16). Это означает, что мы становимся ограничены вычислениями, когда \(2B &gt; 240\) или \(B &gt; 120\). Это намного ниже, что означает: если мы можем квантизовать веса в int8 (что довольно легко сделать), но всё ещё выполнять FLOPs в bfloat16, мы получаем ощутимый выигрыш в эффективности (хотя int8 OPs были бы ещё лучше).</p> </details> <p><strong>Задача 3:</strong> Используя условия из задачи 2, постройте roofline-график пиковой производительности (FLOPs/s) в зависимости от $B$ для $F = D = 4096$ и $F = D = 1024$. <em>Используйте точное количество загружаемых байт, а не приближение.</em></p> <details><summary>Нажмите, чтобы увидеть ответ.</summary> <p>Вот требуемый график:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/roofline-plot-q3-480.webp 480w,/scaling-book/assets/img/roofline-plot-q3-800.webp 800w,/scaling-book/assets/img/roofline-plot-q3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/roofline-plot-q3.png" class="img-fluid img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Обратите внимание, что обе модели в конечном итоге достигают пиковой производительности железа, но при больших D/F это происходит раньше. При D=F=1024 критический размер батча почти удваивается. Код для генерации этого графика:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">bs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">roofline</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">F</span><span class="p">):</span>
  <span class="n">total_flops</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">B</span><span class="o">*</span><span class="n">D</span><span class="o">*</span><span class="n">F</span>
  <span class="n">flops_time</span> <span class="o">=</span> <span class="n">total_flops</span> <span class="o">/</span> <span class="mf">1.97e14</span>
  <span class="n">comms_time</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">B</span><span class="o">*</span><span class="n">D</span> <span class="o">+</span> <span class="n">D</span><span class="o">*</span><span class="n">F</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">B</span><span class="o">*</span><span class="n">F</span><span class="p">)</span> <span class="o">/</span> <span class="mf">8.2e11</span>
  <span class="n">total_time</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">flops_time</span><span class="p">,</span> <span class="n">comms_time</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">total_flops</span> <span class="o">/</span> <span class="n">total_time</span>

<span class="n">roofline_big</span> <span class="o">=</span> <span class="nf">roofline</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="mi">4096</span><span class="p">,</span> <span class="mi">4096</span><span class="p">)</span>
<span class="n">roofline_small</span> <span class="o">=</span> <span class="nf">roofline</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">roofline_big</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">F=D=4096</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">roofline_small</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">F=D=1024</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">batch size</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">peak bfloat16 FLOPs/s on TPU v5e</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">()</span>
</code></pre></div></div> </details> <p><strong>Задача 4:</strong> Что если мы хотим выполнить $\text{int8[B, D]} *_D \text{int8[B, D, F]} \rightarrow \text{int8[B, F]}$, где для каждого элемента батча используется своя матрица. Какова арифметическая интенсивность этой операции?</p> <details><summary>Нажмите, чтобы увидеть ответ.</summary> <p>Начнём с общих FLOPs и коммуникаций.</p> <ol> <li>Всего FLOPs: количество FLOPs в основном то же, поскольку мы выполняем то же количество matmul размером \(BD \times DF\) (подробнее в разделе 4). Итого \(2BDF\).</li> <li>Всего коммуникаций: здесь коммуникаций значительно больше: \(BD + BDF + BF\).</li> <li>Следовательно, наша арифметическая интенсивность теперь равна \(2BDF / (BD + BDF + BF)\). Поскольку \(BDF\) доминирует в знаменателе, это примерно \(2\). То есть вместо зависимости от размера батча интенсивность фактически константна. Это плохо, потому что означает, что мы практически всегда будем ограничены коммуникациями, независимо от параметров.</li> </ol> </details> <p><strong>Задача 5 [Rooflines для GPU]:</strong> Используя <a href="https://www.nvidia.com/en-us/data-center/h100/" rel="external nofollow noopener" target="_blank">спецификацию NVIDIA для H100</a>, рассчитайте размер батча, при котором матричное умножение становится ограниченным вычислениями. <em>Обратите внимание, что значения FLOPs для Tensor Core указаны в два раза больше реального значения, поскольку они достижимы только при структурированной разреженности.</em></p> <details><summary>Нажмите, чтобы увидеть ответ.</summary> <p>Из спецификации видно, что заявленное значение bfloat16 FLOPs составляет <code class="language-plaintext highlighter-rouge">1.979e15</code> FLOPs/s со звёздочкой “с разреженностью”. Реальное значение без разреженности вдвое меньше, то есть около <code class="language-plaintext highlighter-rouge">1e15</code> FLOPs/s. Пропускная способность памяти — 3.35TB/s, или <code class="language-plaintext highlighter-rouge">3.35e12</code> байт/секунду. Таким образом, $B_\text{crit}$ равно <code class="language-plaintext highlighter-rouge">1e15 / 3.35e12 = 298</code>, что довольно близко к TPU.</p> </details> <h3 class="next-section">На этом часть 1 завершена! Для части 2, где рассматривается, как реальные TPU производят вычисления, <a href="../tpus">нажмите здесь</a>.</h3> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> <div class="base-grid appendix-entry"> <h3 style="grid-column: 0;">Miscellaneous</h3> <p class="author-footnote" style="grid-column: text;"><sup>*</sup>Work done at Google DeepMind, now at MatX.</p> </div> <div class="base-grid appendix-entry"> <h3 style="grid-column: 0;">Citation</h3> <p class="author-footnote">For attribution in academic contexts, please cite this work as:</p> <div class="author-footnote"> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c">Austin et al., "How to Scale Your Model", Google DeepMind, online, 2025.</span>
</code></pre></div></div> </div> <p class="author-footnote">or as a BibTeX entry:</p> <div class="author-footnote"> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="nc">@article</span><span class="p">{</span><span class="nl">scaling-book</span><span class="p">,</span>
      <span class="na">title</span> <span class="p">=</span> <span class="s">{How to Scale Your Model}</span><span class="p">,</span>
      <span class="na">author</span> <span class="p">=</span> <span class="s">{Austin, Jacob and Douglas, Sholto and Frostig, Roy and Levskaya, Anselm and Chen, Charlie and Vikram, Sharad
      and Lebron, Federico and Choy, Peter and Ramasesh, Vinay and Webson, Albert and Pope, Reiner}</span><span class="p">,</span>
      <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Google DeepMind}</span><span class="p">,</span>
      <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
      <span class="na">note</span> <span class="p">=</span> <span class="s">{Retrieved from https://jax-ml.github.io/scaling-book/}</span><span class="p">,</span>
      <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
    <span class="p">}</span>
</code></pre></div></div> </div> </div> </d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/main.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'jax-ml/scaling-book',
        'data-repo-id': '',
        'data-category': 'General',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-loading': '1',
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/scaling-book/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/scaling-book/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/scaling-book/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/scaling-book/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/scaling-book/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/scaling-book/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/scaling-book/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/scaling-book/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/scaling-book/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>