---
layout: distill
title: "Как масштабировать свою модель"
subtitle: "Системный взгляд на LLM на TPU"
# permalink: /main/
description: "Обучение больших языковых моделей часто напоминает алхимию, но на самом деле понимать и оптимизировать их работу не так уж сложно. Эта книга объясняет науку масштабирования языковых моделей: как устроены TPU и GPU и как они взаимодействуют между собой, как LLM выполняются на реальном железе, и как распараллелить модели при обучении и инференсе для эффективной работы в огромных масштабах. Если вас когда-нибудь интересовало, во сколько обойдётся обучение конкретной LLM, сколько памяти понадобится для самостоятельного запуска модели, или что вообще такое AllGather то эта книга для вас."
date: 2025-02-04
future: true
htmlwidgets: true
hidden: false

giscus_comments: true

section_number: 0

previous_section_url: ""
previous_section_name: "Часть 0: Введение"

next_section_url: roofline
next_section_name: "Часть 1: Rooflines"

bibliography: main.bib

citation: true

authors:
  - name: Jacob Austin
    url: "https://www.jacobaustin.org/"
    affiliations:
      name: Google DeepMind
  - name: Sholto Douglas
    url: "https://x.com/_sholtodouglas"
  - name: Roy Frostig
    url: "https://cs.stanford.edu/~rfrostig/"
  - name: Anselm Levskaya
    url: "https://anselmlevskaya.com/"
  - name: Charlie Chen
    url: "https://x.com/charliexychen"
  - name: Sharad Vikram
    url: "https://sharadvikram.com/"
  - name: Federico Lebron
    url: "https://fedelebron.com/"
  - name: Peter Choy
    url: "https://x.com/pchoy95"
  - name: Vinay Ramasesh
    url: "https://x.com/vinayramasesh"
  - name: Albert Webson
    url: "https://representation.ai/"
  - name: Reiner Pope<sup>*</sup>
    url: https://x.com/reinerpope

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Общая структура
  - name: Ссылки на разделы

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

{% include figure.liquid path="assets/img/dragon.png" class="img-fluid" %}

Большая часть глубокого обучения до сих пор остаётся чем-то вроде чёрной магии, но оптимизация производительности моделей не должна быть такой же — даже при работе в огромных масштабах! Относительно простые принципы работают везде — от одного ускорителя до десятков тысяч — и их понимание открывает много возможностей:

- Прикинуть, насколько близки части вашей модели к теоретическому оптимуму.
- Осознанно выбирать схемы параллелизма на разных масштабах (как распределить вычисления по множеству устройств).
- Оценить стоимость и время обучения и запуска больших Transformer-моделей.
- Разрабатывать алгоритмы, использующие специфические возможности железа. Примеры: [раз](https://arxiv.org/abs/2205.14135), [два](https://arxiv.org/abs/1911.02150), [три](https://arxiv.org/abs/2007.00072).
- Проектировать железо на основе чёткого понимания того, что ограничивает производительность текущих алгоритмов.

**Требуемые знания:** Мы предполагаем что у вас есть базовое понимание LLM и архитектуры Transformer, и вы еще не разобрались как они работают в масштабе. Вы должны знать основы обучения LLM и в идеале иметь минимальное знакомство с JAX. Полезное фоновое чтение: [этот блог-пост](https://jalammar.github.io/illustrated-transformer/) об архитектуре Transformer и [оригинальная статья](https://arxiv.org/abs/1706.03762). Также посмотрите [этот список](conclusion#further-reading) с дополнительными материалами.

**Цели и обратная связь:** К концу книги вы должны уверенно оценивать оптимальную схему параллелизма для Transformer-модели на конкретном железе и примерно понимать, сколько займёт обучение и инференс. Если что-то непонятно — пишите нам или оставляйте комментарии! Мы хотим знать, как сделать материал понятнее.

<p markdown=1 class="announce">Возможно, вам также понравится новый [Раздел 12](gpus) про GPU от NVIDIA!</p>

### Зачем вам это нужно?

Три-четыре года назад большинству ML-исследователей не требовалось понимать то, о чём эта книга. Но сегодня даже "маленькие" модели работают настолько близко к пределам железа, что любое новое исследование требует думать об эффективности в масштабе.<d-footnote>Исторически ML-исследования следовали циклу tick-tock между системными инновациями и улучшениями софта. Алексу Крижевскому пришлось писать жуткий CUDA-код, чтобы сделать свёрточные сети быстрыми, но через пару лет библиотеки вроде Theano и TensorFlow решили эту проблему. Возможно, то же самое произойдёт и здесь, и всё из этой книги будет абстрагировано через несколько лет. Но законы масштабирования постоянно толкают наши модели к пределам возможностей железа, и похоже, что в обозримом будущем передовые исследования будут неразрывно связаны с пониманием эффективного масштабирования моделей на больших кластерах.</d-footnote> **Выигрыш в 20% на бенчмарках бесполезен, если он съедает 20% roofline.** Многообещающие архитектуры регулярно проваливаются либо потому, что они _не могут_ работать эффективно в масштабе, либо потому, что никто не вкладывает усилия, чтобы это исправить.

> **От переводчика:** Roofline — метод анализа производительности, который показывает теоретический максимум производительности системы как "потолок" (roof + line = линия крыши).

**Цель "масштабирования модели" — увеличивать количество чипов для обучения или инференса с пропорциональным, линейным ростом пропускной способности.** Это называется "сильным масштабированием" (strong scaling). Добавление чипов ("параллелизм") обычно сокращает время вычислений, но требует дополнительной коммуникации между ними. Когда коммуникация занимает больше времени, чем вычисления, мы становимся ограничены коммуникацией (communication-bound) и теряем возможность сильного масштабирования.<d-footnote>По мере сокращения времени вычислений вы также сталкиваетесь с узкими местами на уровне отдельного чипа. Ваш новенький TPU или GPU может быть рассчитан на 500 триллионов операций в секунду, но при неаккуратной работе он легко застрянет на перемещении параметров в памяти и выдаст лишь десятую часть от расчётной производительности. Баланс между вычислениями на чипе, пропускной способностью памяти и её объёмом критичен для масштабирования.</d-footnote> Если мы достаточно хорошо понимаем наше железо, чтобы предвидеть узкие места, мы можем спроектировать или переконфигурировать модели так, чтобы их избежать.<d-footnote>Разработчики железа решают обратную задачу: создать чипы с достаточными вычислениями, пропускной способностью и памятью для наших алгоритмов при минимальной стоимости. Представьте, насколько стрессовая такая совместная разработка: нужно делать ставку на то, как будут выглядеть алгоритмы через 2-3 года, когда чипы станут доступны. В этом плане история TPU была успехом. Матричное умножение уникально тем, что использует гораздо больше операций на байт памяти, чем почти любой другой алгоритм (N операций на байт), и ранние TPU с систолическими массивами достигли гораздо лучшего соотношения производительность/доллар, чем GPU того времени. TPU проектировались под ML-задачи, а GPU с их TensorCore быстро адаптируются. Но представьте, насколько дорого было бы, если бы нейросети не взлетели в популярности или изменились так, что TPU (менее гибкие по природе, чем GPU) не смогли бы справиться.</d-footnote>

Наша цель в этой книге — объяснить, как работает железо TPU и GPU и как архитектура Transformer эволюционировалась для эффективной работы на современном оборудовании. Надеемся, это будет полезно как исследователям, разрабатывающим новые архитектуры, так и инженерам, оптимизирующим текущее поколение LLM.

## Общая структура

Книга построена следующим образом:

[Раздел 1](roofline) объясняет roofline-анализ и факторы, которые могут ограничивать масштабирование: коммуникации, вычисления и память. [Разделы 2](tpus) и [3](sharding) подробно разбирают работу TPU — как отдельных чипов, так и целых взаимосвязанных систем, где связи между чипами имеют ограниченную пропускную способность и задержки. Мы ответим на такие вопросы:

* Сколько должно занимать матричное умножение заданного размера? Когда оно упирается в вычисления, когда в память, а когда в пропускную способность?
* Как TPU объединяются в кластеры для обучения? Какая пропускная способность у разных частей системы?
* Сколько времени занимает gather, scatter или перераспределение массивов между несколькими TPU?
* Как эффективно перемножать матрицы, распределённые по устройствам по-разному?

{% include figure.liquid path="assets/img/pointwise-product.gif" class="img-small" caption="<b>Рисунок:</b> диаграмма из <a href='tpus'>Раздела 2</a>, показывающая, как TPU выполняет поэлементное произведение. В зависимости от размера массивов и пропускной способности различных связей мы можем оказаться ограничены вычислениями (используя полную вычислительную мощность железа) или коммуникацией (с узким местом в загрузке памяти)." %}

Пять лет назад в машинном обучении существовало множество архитектур — ConvNet, LSTM, MLP, трансформеры — но сейчас почти везде используются только трансформеры<d-cite key="transformers"></d-cite>. Мы убеждены, что важно понимать каждый элемент архитектуры трансформера: точные размеры всех матриц, где применяется нормализация, сколько параметров и FLOP<d-footnote>FLoating point OPs — в основном сложения и умножения. Хотя часто FLOP используют для обозначения операций в секунду, мы пишем FLOP/s, чтобы явно это показать.</d-footnote> приходится на каждую часть. В [разделе 4](transformers) мы подробно разбираем эту "математику трансформера" и показываем, как считать параметры и FLOP для обучения и инференса. Это позволяет понять, сколько памяти займёт модель, сколько времени уйдёт на вычисления или коммуникацию, и когда механизм внимания (attention) станет важнее feed-forward блоков.

{% include figure.liquid path="assets/img/transformer-diagram.png" class="img-fluid" caption="<b>Рисунок:</b> стандартный слой Transformer. Каждое матричное умножение (matmul) обозначено точкой в круге. Параметры модели выделены фиолетовым (за исключением нормализаций). Подробный разбор диаграммы — в <a href='transformers'>разделе 4</a>." %}

[Раздел 5: Обучение](training) и [Раздел 7: Инференс](inference) — это сердце книги. В них мы разбираем ключевой вопрос: есть модель определённого размера и какое-то количество чипов — как распараллелить её так, чтобы оставаться в режиме эффективного масштабирования? Вопрос звучит просто, но ответ на него удивительно сложный. Если коротко: существует 4 основных способа параллелизма для распределения моделей по чипам (**по данным**, **по тензорам**, **конвейерный** и **по экспертам**), плюс ещё несколько техник для снижения потребления памяти (**rematerialisation**, **optimizer/model sharding (ZeRO)**, **host offload**, **gradient accumulation**). Многие из них мы здесь и разберём.

К концу этих разделов вы сможете самостоятельно выбирать подходящие подходы для новых архитектур и сценариев. [Раздел 6](applied-training) и [Раздел 8](applied-inference) — это практические руководства, где эти концепции применяются к LLaMA-3, популярной open-source модели.

В [Разделе 9](profiling) и [Разделе 10](jax-stuff) разбирается, как реализовать эти идеи на JAX и как профилировать и дебажить код, когда что-то пошло не так. [Раздел 12](gpus) — свежее дополнение, посвящённое GPU.

По всей книге мы даём задачи для самостоятельной проработки. Не обязательно читать все разделы подряд — выбирайте то, что вам интересно. Будем рады любой обратной связи. Книга пока в статусе черновика и продолжает дорабатываться. Спасибо!

*Благодарим James Bradbury и Blake Hechtman — многие идеи в этой книге основаны на их работе.*

<h3 markdown=1 class="next-section">Итак, [перейдём к Разделу 1](roofline) про roofline-модели TPU.</h3>

## Содержание

Книга получилась довольно объёмной, но надеемся, это вас не отпугнёт. Первые три главы — это введение, их можно пропустить, если тема знакома, хотя там вводятся обозначения, которые используются дальше. Последние три части, пожалуй, самые практически полезные — там разбирается работа с реальными моделями.

**Часть 1: Основы**

* [**Глава 1: Краткое введение в Roofline-анализ**](roofline). Производительность алгоритмов ограничена тремя факторами: вычислениями, передачей данных и памятью. С их помощью можно прикинуть, как быстро будет работать алгоритм.

* [**Глава 2: Как устроены TPU**](tpus). Принцип работы TPU и как это влияет на то, какие модели мы можем обучать и разворачивать.

* [**Глава 3: Шардированные матрицы и их умножение**](sharding). Разбираем шардирование моделей и параллелизм на нескольких TPU на примере нашей любимой операции — перемножения шардированных матриц.

**Часть 2: Трансформеры**

* [**Глава 4: Вся необходимая математика трансформеров**](transformers). Сколько FLOP выполняет трансформер за прямой и обратный проход? Как посчитать количество параметров? Размер KV-кэшей? Разбираем все эти расчёты.

* [**Глава 5: Как распараллелить трансформер для обучения**](training). FSDP. Megatron-шардинг. Конвейерный параллелизм. Есть N чипов — как обучить модель заданного размера с нужным batch size максимально эффективно?

* [**Глава 6: Обучение LLaMA 3 на TPU**](applied-training). Как обучать LLaMA 3 на TPU? Сколько времени это займёт? Во сколько обойдётся?

* [**Глава 7: Всё об инференсе трансформеров**](inference). После обучения модель нужно запустить в production. Инференс добавляет новое ограничение — латентность — и меняет картину с памятью. Разберём, как работает disaggregated serving и как правильно думать о KV-кэшах.

* [**Глава 8: Запуск LLaMA 3 на TPU**](applied-inference). Во сколько обойдётся инференс LLaMA 3 на TPU v5e? Какие есть компромиссы между латентностью и пропускной способностью?

**Часть 3: Практические руководства**

* [**Глава 9: Как профилировать код на TPU**](profiling). Реальные LLM всегда сложнее теории. Здесь мы разбираем стек JAX + XLA и показываем, как использовать профайлер JAX/TensorBoard для отладки и решения реальных проблем.

* [**Глава 10: Программирование TPU на JAX**](jax-stuff). JAX предлагает множество мощных API для параллелизации вычислений, но нужно понимать, как ими пользоваться. Интересные примеры и разобранные задачи.

**Часть 4: Выводы и бонусный контент**

* [**Глава 11: Выводы и дополнительное чтение**](conclusion). Заключительные мысли и дополнительные материалы по TPU и LLM.

* [**Глава 12: Как устроены GPU**](gpus). Бонусный раздел о GPU: как они работают, как объединяются в кластеры, и чем их roofline отличается от TPU.
