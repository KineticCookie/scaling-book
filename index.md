---
layout: distill
title: "Как масштабировать свою модель"
subtitle: "Системный взгляд на LLM на TPU"
# permalink: /main/
description: "Хотя обучение LLM часто похоже на колдоство, понимание и оптимизация производительности ваших моделей на самом деле более прямолинейная задача. Эта книга призвана разобраться этом: как работают TPU (и GPU) и как они общаются друг с другом, как LLM работают на реальном железе, и как распараллеливать ваши модели при обучении и инференсе, чтобы они эффективно работали в больших масштабах. Если вы когда-нибудь задавались вопросом «насколько дорого получится обучение этой LLM» или «сколько памяти мне нужно, чтобы запустить эту модель самому» или «что такое AllGather» то эта книга будет вам полезна."
date: 2025-02-04
future: true
htmlwidgets: true
hidden: false

giscus_comments: true

section_number: 0

previous_section_url: ""
previous_section_name: "Часть 0: Введение"

next_section_url: roofline
next_section_name: "Часть 1: Rooflines"

bibliography: main.bib

citation: true

authors:
  - name: Jacob Austin
    url: "https://www.jacobaustin.org/"
    affiliations:
      name: Google DeepMind
  - name: Sholto Douglas
    url: "https://x.com/_sholtodouglas"
  - name: Roy Frostig
    url: "https://cs.stanford.edu/~rfrostig/"
  - name: Anselm Levskaya
    url: "https://anselmlevskaya.com/"
  - name: Charlie Chen
    url: "https://x.com/charliexychen"
  - name: Sharad Vikram
    url: "https://sharadvikram.com/"
  - name: Federico Lebron
    url: "https://fedelebron.com/"
  - name: Peter Choy
    url: "https://x.com/pchoy95"
  - name: Vinay Ramasesh
    url: "https://x.com/vinayramasesh"
  - name: Albert Webson
    url: "https://representation.ai/"
  - name: Reiner Pope<sup>*</sup>
    url: https://x.com/reinerpope

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Общая структура
  - name: Ссылки на разделы

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

{% include figure.liquid path="assets/img/dragon.png" class="img-fluid" %}

Большая часть глубокого обучения до сих пор остается чем-то вроде черной магии, но оптимизация производительности ваших моделей не обязана быть такой же — даже в огромных масштабах! Относительно простые принципы работают везде — от работы с одним ускорителем до десятков тысяч — и их понимание позволяет делать много полезных вещей:

- Прикинуть, насколько близки части вашей модели к теоретическому оптимуму.
- Осознанно выбирать между разными схемами параллелизма на разных масштабах (как распределить вычисления по множеству устройств).
- Оценить стоимость и время, необходимые для обучения и запуска больших Transformer-моделей.
- Разрабатывать алгоритмы, которые используют специфические возможности железа. Примеры: [раз](https://arxiv.org/abs/2205.14135), [два](https://arxiv.org/abs/1911.02150), [три](https://arxiv.org/abs/2007.00072).
- Проектировать железо, основываясь на четком понимании того, что ограничивает производительность существующих алгоритмов.

**Требуемые знания:** Мы предполагаем, что у вас есть базовое понимание LLM и архитектуры Transformer. Знание того, как они работают в масштабе необязательны. Вы должны знать основы обучения LLM и в идеале иметь минимальное знакомство с JAX. Полезным фоновым чтением может быть [этот блог-пост](https://jalammar.github.io/illustrated-transformer/) об архитектуре Transformer и [оригинальная статья про Transformer](https://arxiv.org/abs/1706.03762). Также посмотрите [этот список](conclusion#further-reading) с дополнительными полезными материалами.

**Цели и обратная связь:** К завершению книги вы должны уверенно оценивать лучшую схему параллелизма для Transformer-модели на конкретной аппаратной платформе и примерно понимать, сколько должно занять обучение и инференс. Если нет — пишите нам или оставляйте комментарии! Мы хотим знать, как рассказать об этих вещах понятнее.

<p markdown=1 class="announce">Возможно, вам также понравится новый [Раздел 12](gpus) про GPU от NVIDIA!</p>

### Зачем вам это нужно?

Три-четыре года назад большинству ML-исследователей не нужно было понимать содержание этой книги. Но сегодня даже "маленькие" модели работают настолько близко к пределам железа, что для новых исследований требуется думать об эффективности в масштабе.<d-footnote>Исторически ML-исследования следовали tick-tock циклу между системными инновациями и улучшениями софта. Алексу Крижевскому пришлось писать жуткий CUDA-код, чтобы сделать CNN быстрыми, но через пару лет библиотеки вроде Theano и TensorFlow решили эту проблему. Может быть, то же самое произойдет здесь, и всё из этой книги будет абстрагировано через несколько лет. Но законы масштабирования постоянно толкают наши модели к границе возможностей нашего железа, и похоже, что в обозримом будущем передовые исследования будут неразрывно связаны с пониманием того, как эффективно масштабировать модели на больших аппаратных топологиях.</d-footnote> **Выигрыш в 20% на бенчмарках бесполезен, если он обходится в 20% потери эффективности по roofline.** Многообещающие архитектуры моделей регулярно проваливаются либо потому, что они _не могут_ работать эффективно в масштабе, либо потому, что никто не вкладывает усилия в то, чтобы это сделать.

> **От переводчика:** Roofline — это метод анализа производительности, который показывает теоретический максимум производительности системы как "потолок" (roof + line = линия крыши).

**Цель "масштабирования модели" — увеличивать количество чипов, используемых для обучения или инференса, получая при этом пропорциональный, линейный рост пропускной способности.** Это называется "*сильным масштабированием*" (strong scaling). Обычно добавление дополнительных чипов ("параллелизм") уменьшает время вычислений, но оно также добавляет затраты на коммуникацию между чипами. Когда коммуникация занимает больше времени, чем вычисления, мы становимся communication-bound (ограничены коммуникацией) и не можем масштабироваться сильно.<d-footnote>По мере уменьшения времени вычислений вы обычно также сталкиваетесь с узкими местами на уровне одного чипа. Ваш новенький TPU или GPU может быть рассчитан на 500 триллионов операций в секунду, но если быть неосторожным, он легко застрянет в перемещении параметров в памяти и сможет делать лишь десятую часть от этого расчета. Взаимодействие вычислений на чипе, пропускной способности памяти и общей памяти критически важно для масштабирования.</d-footnote> Если мы достаточно хорошо понимаем наше железо, чтобы предвидеть, где возникнут эти узкие места, мы можем спроектировать или переконфигурировать наши модели, чтобы их избежать.<d-footnote>Разработчики железа решают обратную задачу: создать железо, которое обеспечивает достаточно вычислений, пропускной способности и памяти для наших алгоритмов при минимизации стоимости. Можете представить, насколько стрессовая такая задача совместной разработки: нужно делать ставку на то, как будут выглядеть алгоритмы, когда первые чипы действительно станут доступны, часто через 2-3 года. История TPU — это огромный успех в этой игре. Матричное умножение — уникальный алгоритм в том смысле, что использует гораздо больше FLOP на байт памяти, чем почти любой другой (N FLOP на байт), и ранние TPU с их архитектурой систолического массива достигли гораздо лучшего соотношения производительность/доллар, чем GPU того времени. TPU были спроектированы для ML-задач, и GPU с их TensorCore быстро меняются, чтобы заполнить эту нишу. Но можете представить, насколько дорого это было бы, если бы нейросети не взлетели или изменились бы так, что TPU (которые по своей природе менее гибкие, чем GPU) не смогли бы справиться.</d-footnote>

*Наша цель в этой книге — объяснить, как работает железо TPU (и GPU) и как архитектура Transformer эволюционировала для оптимальной работы на современном железе. Надеемся, это будет полезно как для исследователей, разрабатывающих новые архитектуры, так и для инженеров, работающих над тем, чтобы заставить текущее поколение LLM работать быстро.*

## Общая структура

Общая структура этой книги следующая:

[Раздел 1](roofline) объясняет roofline-анализ и какие факторы могут ограничивать нашу способность масштабироваться (коммуникация, вычисления и память). [Раздел 2](tpus) и [Раздел 3](sharding) детально рассказывают о том, как работают TPU, как отдельные чипы и — что критически важно — как взаимосвязанная система с межчиповыми связями ограниченной пропускной способности и задержки. Мы ответим на вопросы типа:

* Сколько должно занимать матричное умножение определенного размера? В какой момент оно ограничено вычислениями, памятью или пропускной способностью коммуникации?
* Как TPU соединены вместе, чтобы образовать кластеры для обучения? Какая пропускная способность у каждой части системы?
* Сколько времени занимает gather, scatter или перераспределение массивов по множеству TPU?
* Как эффективно перемножать матрицы, которые по-разному распределены по устройствам?

{% include figure.liquid path="assets/img/pointwise-product.gif" class="img-small" caption="<b>Рисунок:</b> диаграмма из <a href='tpus'>Раздела 2</a>, показывающая, как TPU выполняет поэлементное произведение. В зависимости от размера массивов и пропускной способности различных связей мы можем оказаться ограничены вычислениями (используя полную вычислительную мощность железа) или коммуникацией (с узким местом в загрузке памяти)." %}

Пять лет назад в ML был яркий ландшафт архитектур — ConvNet'ы, LSTM, MLP, Transformer'ы — но сейчас у нас в основном только Transformer<d-cite key="transformers"></d-cite>. Мы твердо убеждены, что стоит понимать каждую часть архитектуры Transformer: точные размеры каждой матрицы, где происходит нормализация, сколько параметров и FLOP<d-footnote>FLoating point OPs, в основном общее количество сложений и умножений. Хотя многие источники используют FLOP в значении "операций в секунду", мы используем FLOP/s, чтобы явно это обозначить.</d-footnote> в каждой части. [Раздел 4](transformers) тщательно проходит по этой "математике Transformer", показывая, как посчитать параметры и FLOP как для обучения, так и для инференса. Это говорит нам, сколько памяти будет использовать наша модель, сколько времени мы потратим на вычисления или коммуникацию, и когда attention станет важным относительно feed-forward блоков.

{% include figure.liquid path="assets/img/transformer-diagram.png" class="img-fluid" caption="<b>Рисунок:</b> стандартный слой Transformer с каждым матричным умножением (matmul), показанным как точка внутри круга. Все параметры (кроме норм) показаны фиолетовым. <a href='transformers'>Раздел 4</a> разбирает эту диаграмму подробнее." %}

[Раздел 5: Обучение](training) и [Раздел 7: Инференс](inference) — ядро этого эссе, где мы обсуждаем фундаментальный вопрос: имея модель определенного размера и некоторое количество чипов, как мне распараллелить модель, чтобы остаться в режиме "сильного масштабирования"? Это простой вопрос с удивительно сложным ответом. На высоком уровне есть 4 основных техники параллелизма, используемых для разделения моделей по множеству чипов (**данные**, **тензоры**, **конвейер** и **эксперты**), и ряд других техник для уменьшения требований к памяти (**rematerialisation**, **optimizer/model sharding (aka ZeRO)**, **host offload**, **gradient accumulation**). Мы обсуждаем многие из них здесь.

Надеемся, к концу этих разделов вы сможете сами выбирать между ними для новых архитектур или настроек. [Раздел 6](applied-training) и [Раздел 8](applied-inference) — практические туториалы, применяющие эти концепции к LLaMA-3, популярной открытой модели.

Наконец, [Раздел 9](profiling) и [Раздел 10](jax-stuff) рассматривают, как реализовать некоторые из этих идей в JAX и как профилировать и отлаживать ваш код, когда что-то идет не так. [Раздел 12](gpus) — новый раздел, который погружается в GPU.

Везде мы стараемся давать вам задачи для самостоятельной работы. Не чувствуйте никакого давления читать все разделы или читать их по порядку. И пожалуйста, оставляйте обратную связь. Пока что это черновик, и он будет продолжать дорабатываться. Спасибо!

*Хотим поблагодарить James Bradbury и Blake Hechtman, которые разработали многие идеи в этом документе.*

<h3 markdown=1 class="next-section">Без лишних слов, [вот Раздел 1](roofline) про roofline'ы TPU.</h3>

## Ссылки на разделы

*Эта серия, возможно, длиннее, чем нужно, но надеемся, это вас не отпугнет. Первые три главы — вводные и могут быть пропущены, если знакомы, хотя они вводят обозначения, используемые позже. Последние три части могут быть наиболее практически полезными, так как объясняют, как работать с реальными моделями.*

**Часть 1: Предварительные знания**

* [**Глава 1: Краткое введение в Roofline-анализ**](roofline). Алгоритмы ограничены тремя вещами: вычислениями, коммуникацией и памятью. Мы можем использовать это, чтобы приблизительно оценить, как быстро будут работать наши алгоритмы.

* [**Глава 2: Как думать о TPU**](tpus). Как работают TPU? Как это влияет на то, какие модели мы можем обучать и запускать?

* [**Глава 3: Шардированные матрицы и как их перемножать**](sharding). Здесь мы объясняем шардирование моделей и многоTPU-параллелизм через нашу любимую операцию: (шардированные) матричные умножения.

**Часть 2: Transformer'ы**

* [**Глава 4: Вся математика Transformer, которую нужно знать**](transformers). Сколько FLOP использует Transformer в прямом и обратном проходе? Можете ли вы посчитать количество параметров? Размер его KV-кэшей? Мы прорабатываем эту математику здесь.

* [**Глава 5: Как распараллелить Transformer для обучения**](training). FSDP. Megatron-шардирование. Pipeline-параллелизм. Имея некоторое количество чипов, как мне обучить модель данного размера с данным размером батча максимально эффективно?

* [**Глава 6: Обучение LLaMA 3 на TPU**](applied-training). Как бы мы обучали LLaMA 3 на TPU? Сколько это заняло бы времени? Сколько это стоило бы?

* [**Глава 7: Всё об инференсе Transformer**](inference). После обучения модели нам нужно её запустить. Инференс добавляет новое соображение — задержку — и меняет ландшафт памяти. Мы поговорим о том, как работает disaggregated serving и как думать о KV-кэшах.

* [**Глава 8: Запуск LLaMA 3 на TPU**](applied-inference). Сколько стоило бы запускать LLaMA 3 на TPU v5e? Какие компромиссы между задержкой и пропускной способностью?

**Часть 3: Практические руководства**

* [**Глава 9: Как профилировать TPU-код**](profiling). Реальные LLM никогда не так просты, как теория выше. Здесь мы объясняем стек JAX + XLA и как использовать профайлер JAX/TensorBoard для отладки и исправления реальных проблем.

* [**Глава 10: Программирование TPU на JAX**](jax-stuff). JAX предоставляет кучу магических API для параллелизации вычислений, но нужно знать, как их использовать. Интересные примеры и разобранные задачи.

**Часть 4: Выводы и бонусный контент**

* [**Глава 11: Выводы и дополнительное чтение**](conclusion). Заключительные мысли и дополнительное чтение про TPU и LLM.

* [**Глава 12: Как думать о GPU**](gpus). Бонусный раздел о GPU, как они работают, как соединены в сеть, и чем их roofline'ы отличаются от TPU.